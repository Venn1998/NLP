{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project 1 _ Coronavirus Fake News\n",
    "\n",
    "Analysis and processing of textual information\n",
    "\n",
    "- Get data from text\n",
    "- Topic detection\n",
    "- Text classification\n",
    "\n",
    "The purpose of this project is to discover characteristic features of fake news about Covid-19 using basic NLP tools. We will also see if it is possible to automatically classify fake news with machine learning methods. We will use the <i>corona_fake.csv</i> dataset. This dataset contains news in English about covid-19 tagged according to whether it is <i>fake</i> news or not. The dataset is organized into four columns: title, text, source, label.\n",
    "\n",
    "We begin using a simple logistic regression model, trained only on the headlines, that reaches accuracy of 84% on the test set. We'll take advantage of the simplicity of the logistic regression model to infer what are the most informative features of fake news.\n",
    "\n",
    "At the end we implement a more complex classifier, using a RNN with LSTM cells and a pretrained embedding layer (Glove). This model will be trained on the full bodies of the news. The accuracy of this model on the test set is comparable with the accuracy of the simpler logistic regression model and it takes much more time to train it. Moreover, being a deep learning model is opaque and we can't see what features the model uses to distinguish between fake and true news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/lorenzovenieri/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"corona_fake.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fake', nan, 'TRUE'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique() #there are some instances of the dataframe with label 'fake' and not 'Fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace(['fake'],'Fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data from texts\n",
    "\n",
    "We'll create 2 datasets: df_fake and df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = df.loc[df['label'] == 'Fake']\n",
    "\n",
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basic protective measures against the new coro...</td>\n",
       "      <td>Stay aware of the latest information on the CO...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Exposing yourself to the sun or to temperature...</td>\n",
       "      <td>You can catch COVID-19, no matter how sunny or...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Being able to hold your breath for 10 seconds ...</td>\n",
       "      <td>The most common symptoms of COVID-19 are dry c...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Drinking alcohol does not protect you against ...</td>\n",
       "      <td>Frequent or excessive alcohol consumption can ...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COVID-19 virus can be transmitted in areas wit...</td>\n",
       "      <td>From the evidence so far, the COVID-19 virus c...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "9   Basic protective measures against the new coro...   \n",
       "14  Exposing yourself to the sun or to temperature...   \n",
       "16  Being able to hold your breath for 10 seconds ...   \n",
       "17  Drinking alcohol does not protect you against ...   \n",
       "18  COVID-19 virus can be transmitted in areas wit...   \n",
       "\n",
       "                                                 text  \\\n",
       "9   Stay aware of the latest information on the CO...   \n",
       "14  You can catch COVID-19, no matter how sunny or...   \n",
       "16  The most common symptoms of COVID-19 are dry c...   \n",
       "17  Frequent or excessive alcohol consumption can ...   \n",
       "18  From the evidence so far, the COVID-19 virus c...   \n",
       "\n",
       "                                               source label  \n",
       "9   https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "14  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "16  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "17  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "18  https://www.who.int/emergencies/diseases/novel...  TRUE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = df.loc[df['label'] == 'TRUE']\n",
    "\n",
    "df_true.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Finding phrases\n",
    "\n",
    "Phrases are multiword terms, that is, sequences of words that have an overall meaning significantly different from the meaning derived from the meanings of the individual words (e.g. New York has a different meaning than can be derived from New and York).\n",
    "\n",
    "We'll compute the best bigrams and trigrams of the fake news headlines that are not in the real news headlines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the list of stopwords from nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#Add some custom stopwords\n",
    "stopwords = stopwords + ['unknown', 've', 'hadn', 'll', 'didn', 'isn', 'doesn', 'hasn' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |  - n_ii counts ``(w1, w2)``, i.e. the bigram being scored\n",
      " |  - n_ix counts ``(w1, *)``\n",
      " |  - n_xi counts ``(*, w2)``\n",
      " |  - n_xx counts ``(*, *)``, i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We compute the tokens of the fake news headlines and abel these tokens by their PoS. (There are news without a headline and there may be headlines with words that have special characters at the beginning, if a news item does not have a headline (NaN in the 'title' column) we substitute NaN for 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitute NaN with 'empty'\n",
    "df_no_na = df.fillna('empty')\n",
    "\n",
    "#dataframe of fake news\n",
    "df_fake = df_no_na.loc[df['label'] == 'Fake']\n",
    "\n",
    "#dataframe of true news\n",
    "df_true = df_no_na.loc[df['label'] == 'TRUE']\n",
    "\n",
    "#toss out empty headlines\n",
    "\n",
    "titles_fake_noempty = [fh for fh in df_fake['title'].to_list() if fh != 'empty']\n",
    "titles_true_noempty = [th for th in df_true['title'].to_list() if th != 'empty']\n",
    "\n",
    "#create a text of all the titles Fake, all in lowercase\n",
    "\n",
    "titles_fake = \" \".join(titles_fake_noempty).lower()\n",
    "\n",
    "#create a text of all the titles True, all in lowercase\n",
    "\n",
    "titles_true = \" \".join(titles_true_noempty).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titulares_fake and true are already lowercase\n",
    "#filter the tokens starting with alphabetic character\n",
    "tokens_titles_fake = [w for w in word_tokenize(titles_fake) if re.match(\"^[a-z]+.*\", w)]\n",
    "tokens_titles_true = [w for w in word_tokenize(titles_true) if re.match(\"^[a-z]+.*\", w)]\n",
    "#PoS labeling\n",
    "tagged_tokens_fake = nltk.pos_tag(tokens_titles_true)\n",
    "tagged_tokens_true = nltk.pos_tag(tokens_titles_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We compute the top 1000 bigrams and the top 1000 trigrams from the labeled tokens (e.g. [(Basic, JJ), ...]) of the false holders. We use the PMI metrics and the Likehood Ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "def good_stw_candidate(candidate):\n",
    "    test = True\n",
    "    if candidate[0][0] in stopwords or candidate[-1][0] in stopwords:\n",
    "        test = False\n",
    "    return test\n",
    "\n",
    "def filter_collocation_candidates(candidates):\n",
    "    #If we loaded a list of stopwords\n",
    "    if len(stopwords) > 0:\n",
    "         #build a list of candidates that are not stopwords\n",
    "        col_candidates_filtered = [c for c in candidates if good_stw_candidate(c) == True]\n",
    "    else:\n",
    "        col_candidates_filtered = candidates\n",
    "    return col_candidates_filtered\n",
    "\n",
    "#get_n_best_candidates and filter for stopwords with the same function\n",
    "def get_n_best_candidates_pmi(bigram_candidates, trigram_candidates, n_best_collocations):\n",
    "    nbest_bigram_candidates = bigram_candidates.nbest(bigram_measures.pmi,n_best_collocations)\n",
    "    nbest_trigram_candidates = trigram_candidates.nbest(trigram_measures.pmi,n_best_collocations)\n",
    "    return filter_collocation_candidates(nbest_bigram_candidates), filter_collocation_candidates(nbest_trigram_candidates)\n",
    "\n",
    "def get_n_best_candidates_lr(bigram_candidates, trigram_candidates, n_best_collocations):\n",
    "    nbest_bigram_candidates = bigram_candidates.nbest(bigram_measures.likelihood_ratio,n_best_collocations)\n",
    "    nbest_trigram_candidates = trigram_candidates.nbest(trigram_measures.likelihood_ratio,n_best_collocations)\n",
    "    return filter_collocation_candidates(nbest_bigram_candidates), filter_collocation_candidates(nbest_trigram_candidates)\n",
    "\n",
    "\n",
    "bigrams_titulares_fake_candidates=BigramCollocationFinder.from_words(tagged_tokens_fake)\n",
    "bigrams_titulares_true_candidates=BigramCollocationFinder.from_words(tagged_tokens_true)\n",
    "trigrams_titulares_fake_candidates=TrigramCollocationFinder.from_words(tagged_tokens_fake)\n",
    "trigrams_titulares_true_candidates=TrigramCollocationFinder.from_words(tagged_tokens_true)\n",
    "\n",
    "nbest_bigram_fake_pmi, nbest_trigram_fake_pmi = get_n_best_candidates_pmi(\n",
    "    bigrams_titulares_fake_candidates, trigrams_titulares_fake_candidates,1000)\n",
    "nbest_bigram_fake_lr, nbest_trigram_fake_lr = get_n_best_candidates_lr(\n",
    "    bigrams_titulares_fake_candidates, trigrams_titulares_fake_candidates,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nbest_bigram_fake_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nbest_bigram_fake_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intersection set contains 554 bigrams\n",
      "197\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "#doing the intersection we can see the distinctive features chosen by both metrics\n",
    "set_pmi=set(nbest_bigram_fake_pmi)\n",
    "set_lr=set(nbest_bigram_fake_lr)\n",
    "intersection=set_pmi.intersection(set_lr)\n",
    "print('The intersection set contains {} bigrams'.format(len(intersection))) \n",
    "\n",
    "#we can also see which bigrams are chosen by one metric and which are chosen by the other\n",
    "pmi_only=set_pmi-set_lr\n",
    "print(len(pmi_only))\n",
    "\n",
    "lr_only=set_lr-set_pmi\n",
    "print(len(lr_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public health',\n",
       " 'tracing help',\n",
       " 'coronavirus disease',\n",
       " 'dryers effective',\n",
       " 'lung disease',\n",
       " 'get sick',\n",
       " 'disease drinking',\n",
       " 'younger people',\n",
       " 'coronavirus spread',\n",
       " 'coronavirus came',\n",
       " 'prevent infection',\n",
       " 'novel coronavirus',\n",
       " 'get tested',\n",
       " 'higher risk',\n",
       " 'really mean',\n",
       " 'coronavirus vaccine',\n",
       " 'potential coronavirus',\n",
       " 'coronavirus epidemic',\n",
       " 'coronavirus outbreak',\n",
       " 'coronavirus compare',\n",
       " 'new coronavirus',\n",
       " 'severe illness',\n",
       " 'older people',\n",
       " 'getting sick',\n",
       " \"n't know\",\n",
       " 'help prevent',\n",
       " 'people also',\n",
       " 'extra food',\n",
       " 'causes covid-19',\n",
       " 'health supplies',\n",
       " 'global health',\n",
       " 'outbreak like',\n",
       " 'treating covid-19',\n",
       " 'coronavirus disease',\n",
       " 'coronavirus crisis',\n",
       " 'get infected',\n",
       " 'could contact',\n",
       " 'outbreak could',\n",
       " 'could help',\n",
       " 'coronavirus disease',\n",
       " 'health care']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see the lr_only bigrams\n",
    "[b[0][0]+' '+b[1][0] for b in lr_only][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['special risks',\n",
       " 'come much',\n",
       " 'pandemic turns',\n",
       " 'women given',\n",
       " 'warmer weather',\n",
       " 'antibodies covid-19',\n",
       " 'system play',\n",
       " 'infecting others',\n",
       " 'convalescent plasma',\n",
       " 'protective measures',\n",
       " 'lab uv',\n",
       " 'pandemic automatically',\n",
       " 'care providers',\n",
       " 'many questions',\n",
       " 'use soap',\n",
       " 'lockdown scientists',\n",
       " 'evidence supporting',\n",
       " 'hot air',\n",
       " 'public wash',\n",
       " 'scientists quantify',\n",
       " 'companies selling',\n",
       " 'evidence points',\n",
       " 'vaccine drug',\n",
       " 'states infectious',\n",
       " 'paper says',\n",
       " 'preventing deaths',\n",
       " 'community statement',\n",
       " 'pandemic experiment',\n",
       " 'become seriously',\n",
       " 'transmitted via',\n",
       " 'war conspiracy',\n",
       " 'covid-19 spread',\n",
       " 'older adults',\n",
       " 'disinfect hands',\n",
       " 'selling fraudulent',\n",
       " 'heat kill',\n",
       " 'outbreaks like',\n",
       " 'approved treatments',\n",
       " 'viral video',\n",
       " 'get hired',\n",
       " 'still come',\n",
       " 'surfaces ventilator',\n",
       " 'well visit',\n",
       " 'pressure medicines',\n",
       " 'health critically',\n",
       " 'avoid exposure',\n",
       " 'system strong',\n",
       " 'keep infected',\n",
       " 'someone blame',\n",
       " 'grocery store',\n",
       " 'food handled',\n",
       " 'paper money',\n",
       " 'origin trending',\n",
       " 'tracing deliver',\n",
       " 'share food',\n",
       " 'kill million',\n",
       " 'vaccines company',\n",
       " 'health decisions',\n",
       " 'food including',\n",
       " 'hair dryers',\n",
       " 'remdesivir effective',\n",
       " 'curve become',\n",
       " 'act like',\n",
       " 'countries spraying',\n",
       " 'child care',\n",
       " 'uv light',\n",
       " 'rules coronavirus',\n",
       " 'man-made novel',\n",
       " 'long road',\n",
       " 'self-isolation rules',\n",
       " 'contagious contact',\n",
       " 'warns companies',\n",
       " 'never go',\n",
       " 'care crosses',\n",
       " 'long list',\n",
       " 'initial vaccines',\n",
       " 'quarantines another',\n",
       " 'public pool',\n",
       " 'contact tracing',\n",
       " 'pandemic terms',\n",
       " 'hand washing',\n",
       " 'spread deciding',\n",
       " 'recover coronavirus',\n",
       " 'fighting disinformation',\n",
       " 'hot weather',\n",
       " 'health advocates',\n",
       " 'stop taking',\n",
       " 'gates wants',\n",
       " 'blood pressure',\n",
       " 'taking hydroxychloroquine',\n",
       " 'disinfectants like',\n",
       " 'wearing medical',\n",
       " 'look like',\n",
       " 'response resources',\n",
       " 'safe sun',\n",
       " 'video spreads',\n",
       " 'spread illness',\n",
       " 'mean humans',\n",
       " 'next explained',\n",
       " 'questions answered',\n",
       " 'old treatment',\n",
       " 'proximal origin',\n",
       " 'use alcohol',\n",
       " 'ineffective treatments',\n",
       " 'health immunomodulation',\n",
       " 'next several',\n",
       " 'social network',\n",
       " 'ill patients',\n",
       " 'quarantine vs.',\n",
       " 'anthony fauci',\n",
       " 'distancing riding',\n",
       " 'health mantra',\n",
       " 'doubtful surfaces',\n",
       " 'help control',\n",
       " 'flu epidemics',\n",
       " 'seek medical',\n",
       " 'pandemic exercise',\n",
       " 'currently available',\n",
       " 'health emergency',\n",
       " 'fraudulent products',\n",
       " 'sick cleaning',\n",
       " 'coronavirus stick',\n",
       " 'use might',\n",
       " 'science says',\n",
       " 'someone sick',\n",
       " 'normal stay',\n",
       " 'medical masks',\n",
       " 'antiviral drug',\n",
       " 'protective antibodies',\n",
       " 'conspiracy theorists',\n",
       " 'fauci discusses',\n",
       " 'may never',\n",
       " 'modern pandemic',\n",
       " 'money uncertainty',\n",
       " 'returning home',\n",
       " 'virtual treatment',\n",
       " 'touching money',\n",
       " 'regular flu',\n",
       " 'adults younger',\n",
       " 'even dying',\n",
       " 'staying safe',\n",
       " 'level far',\n",
       " 'innovative vaccines',\n",
       " 'far alex',\n",
       " 'heat may',\n",
       " 'donate blood',\n",
       " 'key evidence',\n",
       " 'stigma related',\n",
       " 'another disinfectant',\n",
       " 'immunity certificates',\n",
       " 'treatments work',\n",
       " 'quarantine self-isolation',\n",
       " 'borders first',\n",
       " 'many parts',\n",
       " 'potential role',\n",
       " 'countries companies',\n",
       " 'pandemic transcript',\n",
       " 'experts warn',\n",
       " 'serious illness',\n",
       " 'swine flu',\n",
       " 'using quarantines',\n",
       " 'credible evidence',\n",
       " 'spread globally',\n",
       " 'get infected',\n",
       " 'benefit higher',\n",
       " 'help answer',\n",
       " 'europe public',\n",
       " 'physical distancing',\n",
       " 'alternative treatments',\n",
       " 'doctors say',\n",
       " 'vaccine information',\n",
       " \"n't want\",\n",
       " 'stop claiming',\n",
       " 'someone tested',\n",
       " 'packaged food',\n",
       " 'coronaviruses come',\n",
       " 'pet tested',\n",
       " 'frontline health',\n",
       " 'fall even',\n",
       " 'medicines might',\n",
       " 'soap kill',\n",
       " 'practice social',\n",
       " 'country really',\n",
       " 'easily transmitted',\n",
       " 'important thing',\n",
       " 'complicate response',\n",
       " 'cures treatments',\n",
       " 'gets sick',\n",
       " 'pandemic truth',\n",
       " 'name information',\n",
       " 'go back',\n",
       " 'home caring',\n",
       " 'lamp kill',\n",
       " 'higher death',\n",
       " 'vs. isolation',\n",
       " 'certificates false',\n",
       " 'away world']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see the pmi_only bigrams\n",
    "[b[0][0]+' '+b[1][0] for b in pmi_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['theories flourish',\n",
       " 'spreads chasing',\n",
       " 'debunk fringe',\n",
       " 'communications look',\n",
       " 'human consumption',\n",
       " 'summer heat',\n",
       " 'nonurgent appointments',\n",
       " 'antibiotics effective',\n",
       " 'path germany',\n",
       " 'virus outbreak',\n",
       " 'diabetes considered',\n",
       " 'grandkids school',\n",
       " 'plasma therapy',\n",
       " 'uv lamps',\n",
       " 'fever/pollen allergy',\n",
       " 'supply chain',\n",
       " 'spreads many',\n",
       " 'anti-vaxxers already',\n",
       " 'offers financing',\n",
       " 'selling sham',\n",
       " 'accidentally released',\n",
       " 'stay informed',\n",
       " 'grandma still',\n",
       " 'vaccination certificates',\n",
       " 'parents often',\n",
       " 'propagated viral',\n",
       " 'non-outbreak area',\n",
       " 'system act',\n",
       " 'hay fever',\n",
       " 'go away',\n",
       " 'selling fake',\n",
       " 'extremists around',\n",
       " 'reach times',\n",
       " 'ventilator vs.',\n",
       " 'contain pshuttle-sn',\n",
       " 'mutation turbocharge',\n",
       " 'speed fuels',\n",
       " 'experiment shows',\n",
       " 'read different',\n",
       " 'vaccine hesitancy',\n",
       " 'regularly rinsing',\n",
       " 'essential information',\n",
       " 'trade measures',\n",
       " 'drinking methanol',\n",
       " 'shot increases',\n",
       " 'disinfection lamp',\n",
       " 'adding pepper',\n",
       " 'must study',\n",
       " 'create stigma',\n",
       " 'clothes right']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersection bigrams\n",
    "[b[0][0]+' '+b[1][0] for b in intersection][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intersection set contains 64 bigrams\n"
     ]
    }
   ],
   "source": [
    "#trigrams\n",
    "#doing the intersection we can see the distinctive features chosen by both metrics\n",
    "set_pmi_tri=set(nbest_trigram_fake_pmi)\n",
    "set_lr_tri=set(nbest_trigram_fake_lr)\n",
    "intersection_tri=set_pmi_tri.intersection(set_lr_tri)\n",
    "print('The intersection set contains {} bigrams'.format(len(intersection_tri))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people also susceptible',\n",
       " 'contact tracing deliver',\n",
       " 'keep extra food',\n",
       " 'currently no drugs',\n",
       " 'higher than degrees',\n",
       " 'hand dryers effective',\n",
       " 'far alex jones',\n",
       " 'contact tracing help',\n",
       " 'higher death rate',\n",
       " 'lockdown scientists strongly',\n",
       " 'skin thermal scanners',\n",
       " 'alex jones ordered',\n",
       " 'scientists strongly condemn',\n",
       " 'health care workers',\n",
       " 'chronic medical condition',\n",
       " 'least so far',\n",
       " 'certificates false claim',\n",
       " 'celebrities no evidence',\n",
       " 'eating garlic help',\n",
       " 'false conspiracy theories',\n",
       " 'flu shot stock',\n",
       " 'false claim bill',\n",
       " 'learned so far',\n",
       " 'pregnant women given',\n",
       " 'lung disease drinking',\n",
       " 'care workers present',\n",
       " 'nose with saline',\n",
       " 'treatments alex jones',\n",
       " 'difference between self-isolation',\n",
       " 'conspiracy theories flourish',\n",
       " 'strongly condemn rumors',\n",
       " 'serologic antibody testing',\n",
       " 'affect brain function',\n",
       " 'flu shot increases',\n",
       " 'quarantine vs. isolation',\n",
       " 'younger people also',\n",
       " 'rinsing your nose',\n",
       " 'tracing help slow',\n",
       " 'holding your breath',\n",
       " 'otherwise healthy need',\n",
       " 'natural origin trending',\n",
       " 'sequence no evidence',\n",
       " 'coronavirus death rate',\n",
       " 'surfaces sneezes sex',\n",
       " 'humid climates cold',\n",
       " 'anywhere false claim',\n",
       " 'condition which puts',\n",
       " 'hold your breath',\n",
       " 'bill gates calls',\n",
       " 'knew grocery shopping',\n",
       " 'claim bill gates',\n",
       " 'disease drinking alcohol',\n",
       " 'warm weather slow',\n",
       " 'climates cold weather',\n",
       " 'unpacking my groceries',\n",
       " 'conspiracy theories used',\n",
       " 'experts bill gates',\n",
       " 'transmitted through houseflies',\n",
       " 'transmitted through mosquito',\n",
       " 'high-dose vitamin c',\n",
       " 'covid-19 affect brain',\n",
       " 'contain pshuttle-sn sequence',\n",
       " 'bill gates wants',\n",
       " 'bill gates planning']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see the lr_only trigrams\n",
    "[b[0][0]+' '+b[1][0]+' '+b[2][0] for b in intersection_tri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the intersection sets there are all the bigrams and trigrams that are good for both metrics. We can see some bi/trigrams that can be easily associated with fake news ('anti-coronavirus toothpaste','bill gates planning', (maybe) 'wireless technology', ...) but overall they are very diluted between a lot of ngrams that can also be associated with true news.\n",
    "\n",
    "- I can't really tell by the results obtained which metric is the best for our analysis, but from what I've read on the book (Foundations of Statistical Natural Language Processing (Manning & Schutze)) mutual information is a good measure of independence (values close to 0 indicate independence), but it is a bad measure of dependence because for dependence the score depends on the frequency of the individual words. Since we are mesuring pointwise mutual information to generate ngrams (words whose presence is not independent) pmi shouldn't be the wiser choice, so I would pick the likelihood ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most informative bigrams are the ones having the syntactic pattern of a noun phrase, in the cell below we can see how to filter those for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new coronavirus',\n",
       " 'social distancing',\n",
       " 'natural origin',\n",
       " 'novel coronavirus',\n",
       " 'serologic antibody',\n",
       " 'flu shot',\n",
       " 'false claim',\n",
       " 'public health',\n",
       " 'grocery shopping',\n",
       " 'garlic help',\n",
       " 'alex jones',\n",
       " 'death rate',\n",
       " 'humid climates',\n",
       " 'pregnant women',\n",
       " 'pshuttle-sn sequence',\n",
       " 'thermal scanners',\n",
       " 'coronavirus vaccine',\n",
       " 'brain function',\n",
       " 'common cold',\n",
       " 'vitamin c',\n",
       " 'extended stay',\n",
       " 'extraordinary race',\n",
       " 'hot bath',\n",
       " 'specific medicines',\n",
       " 'immune system',\n",
       " 'medical condition',\n",
       " 'claim bill',\n",
       " 'health care',\n",
       " 'public places',\n",
       " 'saline help',\n",
       " 'affect brain',\n",
       " 'immune system',\n",
       " 'lung disease',\n",
       " 'coronavirus epidemic',\n",
       " 'extra food',\n",
       " 'severe illness',\n",
       " 'anti-coronavirus toothpaste',\n",
       " 'anti-vaccine movement',\n",
       " 'biocontainment room',\n",
       " 'biological threats',\n",
       " 'breathing problem',\n",
       " 'calm carry',\n",
       " 'cautious consider',\n",
       " 'claiming silver',\n",
       " 'cross-border travel',\n",
       " 'cytokine storms',\n",
       " 'dark web',\n",
       " 'digital handshake',\n",
       " 'disinfection lamp',\n",
       " 'dr. anthony',\n",
       " 'early herd',\n",
       " 'eastern france',\n",
       " 'elective surgery',\n",
       " 'elusive dream',\n",
       " 'ethical considerations',\n",
       " 'far-right extremists',\n",
       " 'ferocious rampage',\n",
       " 'fertile corners',\n",
       " 'fringe theory',\n",
       " 'genomic study',\n",
       " 'german hospitals',\n",
       " 'grandkids school',\n",
       " 'hesitancy post-covid-19',\n",
       " 'hospital biocontainment',\n",
       " 'international concern',\n",
       " 'laboratory engineering',\n",
       " 'memory fade',\n",
       " 'methanol ethanol',\n",
       " 'microchip implants',\n",
       " 'misconception genomic',\n",
       " 'mobile networks',\n",
       " 'multiple waves',\n",
       " 'nasal swab',\n",
       " 'nonurgent appointments',\n",
       " 'open windows',\n",
       " 'plasma donors',\n",
       " 'prize-winning immunologist',\n",
       " 'real solution',\n",
       " 'related respiratory',\n",
       " 'smoking vaping',\n",
       " 'spread—and disprove',\n",
       " 'supply chain',\n",
       " 'therapeutic options',\n",
       " 'toothpaste f.d.a',\n",
       " 'transcript dr.',\n",
       " 'truth tracker',\n",
       " 'u.s. government',\n",
       " 'u.s.-ukraine partnership',\n",
       " 'ultraviolet disinfection',\n",
       " 'uncertain path',\n",
       " 'underlying conditions',\n",
       " 'unknowns complicate',\n",
       " 'update fda',\n",
       " 'worst-case scenario',\n",
       " 'york city',\n",
       " 'coronavirus disease',\n",
       " 'potential coronavirus',\n",
       " 'accuracy anti-vaxxers',\n",
       " 'big thing',\n",
       " 'black celebrities',\n",
       " 'close contact',\n",
       " 'experimental therapies',\n",
       " 'face masks',\n",
       " 'france germany',\n",
       " 'harmful disinformation',\n",
       " 'hay fever',\n",
       " 'human consumption',\n",
       " 'human speech',\n",
       " 'key indicators',\n",
       " 'light injections',\n",
       " 'lost sense',\n",
       " 'main pieces',\n",
       " 'major challenges',\n",
       " 'non-outbreak area',\n",
       " 'path germany',\n",
       " 'recreational sports',\n",
       " 'schengen area',\n",
       " 'self-isolation self-quarantine',\n",
       " 'several days',\n",
       " 'several weeks',\n",
       " 'super spreaders',\n",
       " 'typical hay',\n",
       " 'uncertainty investigation',\n",
       " 'united states',\n",
       " 'uv lamps',\n",
       " 'web anti-vaxxers',\n",
       " 'wild animals',\n",
       " 'wireless technology',\n",
       " 'global health',\n",
       " 'contact tracers',\n",
       " 'current speed',\n",
       " 'different approaches',\n",
       " 'different estimates',\n",
       " 'herd immunity',\n",
       " 'high-dose vitamin',\n",
       " 'hygienic measures',\n",
       " 'infectious diseases',\n",
       " 'long-term immunity',\n",
       " 'n.y.c deaths',\n",
       " 'normal level',\n",
       " 'plasma therapy',\n",
       " 'r number',\n",
       " 'record speed',\n",
       " 'reproductive number',\n",
       " 'respirator quarantine',\n",
       " 'saunas hair',\n",
       " 'special report',\n",
       " 'tracker video',\n",
       " 'warp speed',\n",
       " 'world seize',\n",
       " 'coronavirus outbreak',\n",
       " 'anthony fauci',\n",
       " 'case series',\n",
       " 'cryptic transmission',\n",
       " 'essential information',\n",
       " 'human-to-human transmission',\n",
       " 'information war',\n",
       " 'intelligence community',\n",
       " 'outbreak become',\n",
       " 'primary case',\n",
       " 'seasonal cycle',\n",
       " 'seasonal influenza',\n",
       " 'shot stock',\n",
       " 'stressful fauci',\n",
       " 'summer heat',\n",
       " 'travel-related case',\n",
       " 'viral illnesses',\n",
       " 'viral load',\n",
       " 'wuhan lab',\n",
       " 'blood transfusion',\n",
       " 'blood type',\n",
       " 'certain blood',\n",
       " 'covid cure',\n",
       " 'sun exposure',\n",
       " 'coronavirus compare',\n",
       " 'coronavirus crisis',\n",
       " 'average person',\n",
       " 'dangerous misconception',\n",
       " 'global cooperation',\n",
       " 'global coordination',\n",
       " 'hand sanitizer',\n",
       " 'herbal drugs',\n",
       " 'medical equipment',\n",
       " 'postal system',\n",
       " 'sars-cov-2 https',\n",
       " 'system act',\n",
       " 'whole body',\n",
       " 'face mask',\n",
       " 'healthy need',\n",
       " 'much distance']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to filter bigrams by JJ+NN/NNS, NN+NN we can implement this function:\n",
    "def PoS_filter_bigrams(tuple_list):\n",
    "    filtered=[]\n",
    "    for t in tuple_list:\n",
    "        if (t[0][1]=='JJ' and (t[1][1]=='NN' or t[1][1]=='NNS')) or (t[0][1]=='NN' and t[1][1]=='NN'):\n",
    "            filtered.append(t)\n",
    "    return filtered\n",
    "    \n",
    "[b[0][0]+' '+b[1][0] for b in PoS_filter_bigrams(nbest_bigram_fake_lr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and phrases vectorization (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get 'coronavirus'-related terms in the fake news and 'coronavirus'-related terms in the real news. We'll use the semantic similarity calculation of a word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We train a phrase detection model in a sentence. For training we use all the headlines and bodies (not empty) of the false and true news.We uss Gensim's Phraser module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.concat([df_fake, df_true], axis=0, ignore_index=True)\n",
    "\n",
    "text_stream = [word_tokenize(d.lower()) for d in df.title.tolist()+df.text.tolist() \n",
    "               if word_tokenize(d.lower())!=[] ]\n",
    "phrases = Phrases(text_stream, min_count=1, threshold=2, delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We transform each phrase of the fake news in a list of phrases lemmatized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_stw_candidate(t):\n",
    "    v = False\n",
    "    #If it's composed by more than one word\n",
    "    if ' ' in t:\n",
    "        tl = t.split(' ') #list of tokens\n",
    "        #If the first and last tokens are alphabetic and not stopwords\n",
    "        if re.match(\"^[a-z]+.*\", tl[0]) and re.match(\"^[a-z]+.*\", tl[-1]) and \\\n",
    "           tl[0] not in stopwords and tl[1] not in stopwords:\n",
    "            v = True\n",
    "    #If it's composed by only one word\n",
    "    else:\n",
    "        #If it's alphabetic and not a stopword\n",
    "        if t not in stopwords and re.match(\"^[a-z]+.*\", t):\n",
    "            v = True\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the NLTK lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#To obtain the lemma, the term must have a PoS tag. The format of the Wordnet PoS tag is different from the\n",
    "#tag of the PoS tagger of NLTK\n",
    "\n",
    "def get_wn_pos(pos): #translate PoS tagger format to Wordnet format\n",
    "    if re.match(r'^N',pos):\n",
    "        wn_pos = 'n'\n",
    "    elif re.match(r'^V',pos):\n",
    "        wn_pos = 'v'\n",
    "    else:\n",
    "        wn_pos = 'n'\n",
    "    return wn_pos\n",
    "\n",
    "def wnlemmatize(t,postag): #lemmatizes a term with a PoS tag using Wordnet\n",
    "    lemma = \"\"\n",
    "    lem = WordNetLemmatizer() #create a lemmatizer\n",
    "    #If the candidate is a single word, get the lemma with the Wordnet lemmatizer according to its PoS\n",
    "    if ' ' not in t:\n",
    "        lemma = lem.lemmatize(t,get_wn_pos(postag[0][1]))\n",
    "    #If the candidate is a multiword, get the lemma as if it were a name, applying the lemmatizer of WordNet\n",
    "    else:\n",
    "        lemma = lem.lemmatize(t,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['recent outbreak', 'covid-19', 'health organization', 'give', 'away', 'vaccine', 'kit', 'pay', 'shipping'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['china', 'seek', 'court', 'approval', 'coronavirus patients'], ['empty'], ['coronavirus originate', 'chinese government', 'laboratory', 'scientists believe', 'killer', 'disease', 'begin', 'research facility', 'wet', 'fish market'], ['empty'], ['coronavirus outbreak', 'criminal', 'preplanned action'], ['people learned', 'coincidence'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['empty'], ['covid', 'terror', 'uncertainty'], ['covid-19', 'certain', 'state', 'state', 'control'], ['empty'], ['empty'], ['empty'], ['covid-19', 'climate alarmists', 'fear', 'is…'], ['empty'], ['empty'], ['empty'], ['empty'], ['southern', 'europe', 'slave market'], ['coronavirus', 'reactionary', 'usa'], ['empty'], ['empty'], ['empty'], ['empty'], ['bill gates', 'world economic', 'forum predict', 'coronavirus outbreak', 'inside look', 'may'], ['super flu', 'cold'], ['panic', 'steal', 'wealth', 'cold'], ['virus escaped', 'chinese lab']]\n"
     ]
    }
   ],
   "source": [
    "df_fake = df.loc[df['label'] == 'Fake']\n",
    "\n",
    "postag={}\n",
    "\n",
    "def transform_sentence(doc):\n",
    "    doc_tokens = word_tokenize(doc.lower()) #tokenize the sentence\n",
    "    text_phrases = phrases[doc_tokens] #get the phrases\n",
    "    #Toss out punctuation from the phrases in the titles\n",
    "    phrases_stripped = [tp.strip('\".,;:-():!?-‘’ ') for tp in text_phrases]\n",
    "    #remove the empty strings\n",
    "    phrases_stripped = [t for t in phrases_stripped if t!='']\n",
    "    #filter for stw_goodcandidates\n",
    "    phrases_no_stw = [tp for tp in phrases_stripped if good_stw_candidate(tp) == True]\n",
    "    #PoS labeling of the phrases, store them in the postag dict\n",
    "    for ps in phrases_no_stw:\n",
    "        postag[ps] = nltk.pos_tag(word_tokenize(ps))\n",
    "    #lemmatize phrases with Wordnet\n",
    "    phrases_lemmatized = [wnlemmatize(ps,postag[ps]) for ps in phrases_no_stw]    \n",
    "    return phrases_lemmatized\n",
    "\n",
    "sentence_stream = df_fake.title.tolist() + df_fake.text.tolist()\n",
    "transformed_sentences = [transform_sentence(ss) for ss in sentence_stream]\n",
    "\n",
    "\n",
    "print(transformed_sentences[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We build a Word2Vec model of the titles and bodies of fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(897019, 1172630)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vnyt = gensim.models.Word2Vec(\n",
    "        transformed_sentences,\n",
    "        vector_size=150, # size of the vectors in the model\n",
    "        window=10, # size of the context window \n",
    "        min_count= 3, # minimum number of times a word must appear to be included in the model\n",
    "        workers= 1, # number of worker threads to use when training the model\n",
    "        seed=1 \n",
    ")\n",
    "\n",
    "w2vnyt.train(transformed_sentences, total_examples=len(transformed_sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Select the vocabulary of the word2vec model on which the similar terms will be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavirus', 'china', 'virus', 'covid-19', 'people', 'vaccine', 'state', 'say', 'vitamin c', 'time', 'include', 'disease', 'use', 'pandemic', 'wuhan', 'bill gates', 'death', 'government', 'u', 'research', 'case', 'infection', 'make', 'system', 'report', 'sars', 'get', 'work', 'flu', 'lab', 'human', 'year', 'many', 'control', 'study', 'medium', 'take', 'health', 'country', 'world', 'see', 'new', 'cause', 'test', 'hospital', 'accord', 'c', 'patient', 'day', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "phrases_vocabulary = list(w2vnyt.wv.key_to_index)\n",
    "\n",
    "#list of PoS tags that cannot be the initial or final token of a term\n",
    "no_pos_in = ['DT', 'IN', 'PRP', 'CC', 'CD','MD', 'VBG', 'VBD', 'RP', 'RB'] \n",
    "\n",
    "def good_candidate(t,postag):\n",
    "    v = False\n",
    "    #If it's composed by more than one word\n",
    "    if ' ' in t:\n",
    "        tl = t.split(' ') # list of tokens\n",
    "        #If the first and last tokens are alphabetic and not stopwords\n",
    "        if re.match(\"^[a-z]+.*\", tl[0]) and re.match(\"^[a-z]+.*\", tl[-1]) and \\\n",
    "            tl[0] not in stopwords and tl[1] not in stopwords:\n",
    "            # and its PoS tag is not in the no_pos_in list\n",
    "            if postag[0][1] not in no_pos_in and postag[-1][1] not in no_pos_in:\n",
    "                v = True\n",
    "    #If it's composed by only one word \n",
    "    else:\n",
    "        #If it's alphabetic and not a stopword\n",
    "        if t not in stopwords and re.match(\"^[a-z]+.*\", t):\n",
    "            # and its PoS tag is not in the no_pos_in list\n",
    "            if postag[0][1] not in no_pos_in:\n",
    "                v = True\n",
    "    return v\n",
    "\n",
    "def phrase_is_term(phrase): #checks if a phrase is a term\n",
    "    test = False\n",
    "    if phrase not in postag: \n",
    "        pos = nltk.pos_tag(word_tokenize(phrase)) \n",
    "    else:\n",
    "        pos = postag[phrase]\n",
    "    if good_candidate(phrase,pos):\n",
    "        test = True\n",
    "    return test\n",
    "\n",
    "terms_vocabulary = [pv for pv in phrases_vocabulary if phrase_is_term(pv) == True]\n",
    "\n",
    "print(terms_vocabulary[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Let's visualize the tems (from fake news) semantically closest to 'coronavirus'\n",
    "\n",
    "Considering the terms as feature names, the feature name-distance relationship is expressed as a tuple.  \n",
    "The first element of the tuple is the feature name and the second element is its distance value to a term of reference according to #Word2Vec. These tuples are placed in a list (w2v_tuples) so that later can be ordered from closest to least close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distancia respecto al término coronavirus\n",
      "\n",
      "                       Term  Distance_fake\n",
      "0                      gate       0.008447\n",
      "1        roseland community       0.035509\n",
      "2                government       0.043602\n",
      "3     investigational drugs       0.048055\n",
      "4                      take       0.050527\n",
      "...                     ...            ...\n",
      "6543     natural reservoirs       0.935428\n",
      "6544         horseshoe bats       0.945813\n",
      "6545     deadliest diseases       0.946683\n",
      "6546         h5n1 influenza       0.949906\n",
      "6547          includes sars       0.953583\n",
      "\n",
      "[6548 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "term = 'coronavirus'\n",
    "\n",
    "w2v_tuples = []\n",
    "\n",
    "feature_names = terms_vocabulary\n",
    "\n",
    "#For each feature name, we calculate its distance with the reference term using the model.similarity method.\n",
    "#If the distance is bigger than 0, the tuple is put in the list of tuples\n",
    "for i in range(0, len(feature_names)):\n",
    "    if feature_names[i] != term and w2vnyt.wv.similarity(term, feature_names[i]) > 0:\n",
    "        w2v_tuples.append((feature_names[i], w2vnyt.wv.similarity(term, feature_names[i])))\n",
    "    \n",
    "#We sort the list of tuples by the distance\n",
    "w2v_sorted_tuples = sorted(w2v_tuples, key=lambda tup: tup[1])\n",
    "\n",
    "#print(w2v_sorted_tuples)\n",
    "\n",
    "labels = ['Term', 'Distance_fake']\n",
    "\n",
    "#we create a dataframe from which we will build the table\n",
    "df4 = pd.DataFrame.from_records(w2v_sorted_tuples, columns=labels)\n",
    "\n",
    "#Construction and visualization of the table\n",
    "print (\"\")\n",
    "print (\"Distancia respecto al término\", term) \n",
    "print (\"\")\n",
    "\n",
    "print (df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We do the same for true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distancia respecto al término coronavirus\n",
      "\n",
      "                      Term  Distance_true\n",
      "0              cause death       0.050547\n",
      "1                  vaccine       0.091737\n",
      "2        develops symptoms       0.135857\n",
      "3     severe complications       0.150809\n",
      "4                      use       0.208946\n",
      "...                    ...            ...\n",
      "5578               frieman       0.971916\n",
      "5579           common cold       0.972364\n",
      "5580                rarely       0.977768\n",
      "5581          severe viral       0.983238\n",
      "5582                 cause       0.993837\n",
      "\n",
      "[5583 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "postag={}\n",
    "sentence_stream_true=df_true.title.tolist()+df_true.text.tolist()\n",
    "transformed_sentences_true = [transform_sentence(ss) for ss in sentence_stream_true]\n",
    "\n",
    "w2vnyt_true = gensim.models.Word2Vec(\n",
    "        transformed_sentences_true,\n",
    "        vector_size=150, # size of the vectors in the model\n",
    "        window=10, #context window size\n",
    "        min_count= 3, # minimum number of times a word must appear to be included in the model\n",
    "        workers= 1, # number of worker threads to use when training the model\n",
    "        seed=1 \n",
    ")\n",
    "\n",
    "w2vnyt_true.train(transformed_sentences_true, total_examples=len(transformed_sentences_true), epochs=10)\n",
    "\n",
    "phrases_vocabulary_true = list(w2vnyt_true.wv.key_to_index)\n",
    "\n",
    "term = 'coronavirus'\n",
    "\n",
    "w2v_true_tuples = []\n",
    "\n",
    "feature_names_true = phrases_vocabulary_true\n",
    "\n",
    "#For each feature name, we calculate its distance with the reference term using the model.similarity method.\n",
    "for i in range(0, len(feature_names_true)):\n",
    "    if feature_names_true[i] != term and w2vnyt_true.wv.similarity(term, feature_names_true[i]) > 0:\n",
    "        w2v_true_tuples.append((feature_names_true[i], w2vnyt_true.wv.similarity(term, feature_names_true[i])))\n",
    "\n",
    "#We sort the list of tuples by the distance\n",
    "w2v_true_sorted_tuples = sorted(w2v_true_tuples, key=lambda tup: tup[1])\n",
    "\n",
    "#print(w2v_true_sorted_tuples)\n",
    "\n",
    "labels = ['Term', 'Distance_true']\n",
    "\n",
    "#we create a dataframe from which we will build the table\n",
    "df5 = pd.DataFrame.from_records(w2v_true_sorted_tuples, columns=labels)\n",
    "\n",
    "#Construction and visualization of the table\n",
    "print (\"\")\n",
    "print (\"Distancia respecto al término\", term) \n",
    "print (\"\")\n",
    "\n",
    "print (df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance_fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gate</td>\n",
       "      <td>0.008447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roseland community</td>\n",
       "      <td>0.035509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>government</td>\n",
       "      <td>0.043602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigational drugs</td>\n",
       "      <td>0.048055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take</td>\n",
       "      <td>0.050527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>system</td>\n",
       "      <td>0.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>u</td>\n",
       "      <td>0.057408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad news</td>\n",
       "      <td>0.063913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>control</td>\n",
       "      <td>0.071339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>docuseries</td>\n",
       "      <td>0.076788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Term  Distance_fake\n",
       "0                   gate       0.008447\n",
       "1     roseland community       0.035509\n",
       "2             government       0.043602\n",
       "3  investigational drugs       0.048055\n",
       "4                   take       0.050527\n",
       "5                 system       0.054600\n",
       "6                      u       0.057408\n",
       "7               bad news       0.063913\n",
       "8                control       0.071339\n",
       "9             docuseries       0.076788"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cause death</td>\n",
       "      <td>0.050547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>0.091737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>develops symptoms</td>\n",
       "      <td>0.135857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>severe complications</td>\n",
       "      <td>0.150809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>0.208946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meter</td>\n",
       "      <td>0.224472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fast-tracking</td>\n",
       "      <td>0.230735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>diagnostic</td>\n",
       "      <td>0.237897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>marston</td>\n",
       "      <td>0.239012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hand</td>\n",
       "      <td>0.257027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Term  Distance_true\n",
       "0           cause death       0.050547\n",
       "1               vaccine       0.091737\n",
       "2     develops symptoms       0.135857\n",
       "3  severe complications       0.150809\n",
       "4                   use       0.208946\n",
       "5                 meter       0.224472\n",
       "6         fast-tracking       0.230735\n",
       "7            diagnostic       0.237897\n",
       "8               marston       0.239012\n",
       "9                  hand       0.257027"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we can see the terms ordered by how much their distance with the term 'coronavirus' differs from the fake news set and the true news set. More difference means that the term is interestingly more representative of one of the two classes. \n",
    "\n",
    "One thing that is noticeable is how much the words related to the Bill & Melinda Gates Foundation (the term 'gate' may be the lemmatization of 'Gates')) are closer to 'coronavirus' in the fake news set compared to the true news. This gets along with our experience since there are a lot of conspiracy theories about the Gates foundation and Coronavirus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Distance_fake</th>\n",
       "      <th>Distance_true</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bad news</td>\n",
       "      <td>0.063913</td>\n",
       "      <td>0.863050</td>\n",
       "      <td>0.799137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gate</td>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.666269</td>\n",
       "      <td>0.657822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>people</td>\n",
       "      <td>0.092993</td>\n",
       "      <td>0.739906</td>\n",
       "      <td>0.646913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>control</td>\n",
       "      <td>0.071339</td>\n",
       "      <td>0.697567</td>\n",
       "      <td>0.626229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>chose</td>\n",
       "      <td>0.151664</td>\n",
       "      <td>0.726912</td>\n",
       "      <td>0.575248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>system</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.626957</td>\n",
       "      <td>0.572358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>world</td>\n",
       "      <td>0.213319</td>\n",
       "      <td>0.767957</td>\n",
       "      <td>0.554638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>much lower</td>\n",
       "      <td>0.156879</td>\n",
       "      <td>0.705466</td>\n",
       "      <td>0.548587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>power</td>\n",
       "      <td>0.110879</td>\n",
       "      <td>0.658943</td>\n",
       "      <td>0.548064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>foundation</td>\n",
       "      <td>0.082848</td>\n",
       "      <td>0.616305</td>\n",
       "      <td>0.533456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>government</td>\n",
       "      <td>0.043602</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>0.523751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>government agencies</td>\n",
       "      <td>0.160137</td>\n",
       "      <td>0.677353</td>\n",
       "      <td>0.517216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u</td>\n",
       "      <td>0.057408</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.510033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>melinda gates</td>\n",
       "      <td>0.120841</td>\n",
       "      <td>0.627121</td>\n",
       "      <td>0.506280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vaccination</td>\n",
       "      <td>0.158036</td>\n",
       "      <td>0.660445</td>\n",
       "      <td>0.502409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>toll</td>\n",
       "      <td>0.228213</td>\n",
       "      <td>0.717986</td>\n",
       "      <td>0.489773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>closing</td>\n",
       "      <td>0.180088</td>\n",
       "      <td>0.643422</td>\n",
       "      <td>0.463334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>diagnostic</td>\n",
       "      <td>0.698117</td>\n",
       "      <td>0.237897</td>\n",
       "      <td>0.460220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>population</td>\n",
       "      <td>0.310554</td>\n",
       "      <td>0.762980</td>\n",
       "      <td>0.452426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>digital</td>\n",
       "      <td>0.196397</td>\n",
       "      <td>0.645358</td>\n",
       "      <td>0.448961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>life</td>\n",
       "      <td>0.196320</td>\n",
       "      <td>0.643759</td>\n",
       "      <td>0.447439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>severe complications</td>\n",
       "      <td>0.587397</td>\n",
       "      <td>0.150809</td>\n",
       "      <td>0.436588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>medium</td>\n",
       "      <td>0.238500</td>\n",
       "      <td>0.667542</td>\n",
       "      <td>0.429042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mass gatherings</td>\n",
       "      <td>0.217618</td>\n",
       "      <td>0.644867</td>\n",
       "      <td>0.427249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>hopkins center</td>\n",
       "      <td>0.284154</td>\n",
       "      <td>0.710281</td>\n",
       "      <td>0.426127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>unidentified</td>\n",
       "      <td>0.484916</td>\n",
       "      <td>0.907835</td>\n",
       "      <td>0.422919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>church</td>\n",
       "      <td>0.414844</td>\n",
       "      <td>0.836995</td>\n",
       "      <td>0.422151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>society</td>\n",
       "      <td>0.223155</td>\n",
       "      <td>0.641169</td>\n",
       "      <td>0.418013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>sars-cov-2 virus</td>\n",
       "      <td>0.466042</td>\n",
       "      <td>0.879672</td>\n",
       "      <td>0.413631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>influence</td>\n",
       "      <td>0.268647</td>\n",
       "      <td>0.682028</td>\n",
       "      <td>0.413380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>fundamental</td>\n",
       "      <td>0.490767</td>\n",
       "      <td>0.903265</td>\n",
       "      <td>0.412498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.293552</td>\n",
       "      <td>0.705589</td>\n",
       "      <td>0.412037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>politician</td>\n",
       "      <td>0.373746</td>\n",
       "      <td>0.783666</td>\n",
       "      <td>0.409920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>test</td>\n",
       "      <td>0.691577</td>\n",
       "      <td>0.289005</td>\n",
       "      <td>0.402572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>flu</td>\n",
       "      <td>0.387179</td>\n",
       "      <td>0.781119</td>\n",
       "      <td>0.393941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>invest</td>\n",
       "      <td>0.259269</td>\n",
       "      <td>0.650016</td>\n",
       "      <td>0.390747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>fever</td>\n",
       "      <td>0.832818</td>\n",
       "      <td>0.442447</td>\n",
       "      <td>0.390371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>genetics</td>\n",
       "      <td>0.668181</td>\n",
       "      <td>0.281013</td>\n",
       "      <td>0.387168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.243591</td>\n",
       "      <td>0.629304</td>\n",
       "      <td>0.385713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>unfold</td>\n",
       "      <td>0.527646</td>\n",
       "      <td>0.910649</td>\n",
       "      <td>0.383002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>global vaccine</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>0.672388</td>\n",
       "      <td>0.382940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>way</td>\n",
       "      <td>0.296122</td>\n",
       "      <td>0.678856</td>\n",
       "      <td>0.382735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>health security</td>\n",
       "      <td>0.321791</td>\n",
       "      <td>0.703496</td>\n",
       "      <td>0.381705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>force</td>\n",
       "      <td>0.312763</td>\n",
       "      <td>0.693235</td>\n",
       "      <td>0.380472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>disease outbreak</td>\n",
       "      <td>0.540486</td>\n",
       "      <td>0.920340</td>\n",
       "      <td>0.379854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>logic</td>\n",
       "      <td>0.433504</td>\n",
       "      <td>0.810863</td>\n",
       "      <td>0.377358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>country</td>\n",
       "      <td>0.242355</td>\n",
       "      <td>0.619393</td>\n",
       "      <td>0.377038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>good health</td>\n",
       "      <td>0.338167</td>\n",
       "      <td>0.714336</td>\n",
       "      <td>0.376169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>emergency</td>\n",
       "      <td>0.294177</td>\n",
       "      <td>0.666495</td>\n",
       "      <td>0.372318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>gates foundation</td>\n",
       "      <td>0.292785</td>\n",
       "      <td>0.663574</td>\n",
       "      <td>0.370789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term  Distance_fake  Distance_true  Difference\n",
       "5                 bad news       0.063913       0.863050    0.799137\n",
       "0                     gate       0.008447       0.666269    0.657822\n",
       "9                   people       0.092993       0.739906    0.646913\n",
       "6                  control       0.071339       0.697567    0.626229\n",
       "13                   chose       0.151664       0.726912    0.575248\n",
       "3                   system       0.054600       0.626957    0.572358\n",
       "21                   world       0.213319       0.767957    0.554638\n",
       "14              much lower       0.156879       0.705466    0.548587\n",
       "10                   power       0.110879       0.658943    0.548064\n",
       "7               foundation       0.082848       0.616305    0.533456\n",
       "1               government       0.043602       0.567352    0.523751\n",
       "16     government agencies       0.160137       0.677353    0.517216\n",
       "4                        u       0.057408       0.567442    0.510033\n",
       "11           melinda gates       0.120841       0.627121    0.506280\n",
       "15             vaccination       0.158036       0.660445    0.502409\n",
       "24                    toll       0.228213       0.717986    0.489773\n",
       "17                 closing       0.180088       0.643422    0.463334\n",
       "2363            diagnostic       0.698117       0.237897    0.460220\n",
       "47              population       0.310554       0.762980    0.452426\n",
       "19                 digital       0.196397       0.645358    0.448961\n",
       "18                    life       0.196320       0.643759    0.447439\n",
       "1440  severe complications       0.587397       0.150809    0.436588\n",
       "25                  medium       0.238500       0.667542    0.429042\n",
       "22         mass gatherings       0.217618       0.644867    0.427249\n",
       "38          hopkins center       0.284154       0.710281    0.426127\n",
       "328           unidentified       0.484916       0.907835    0.422919\n",
       "126                 church       0.414844       0.836995    0.422151\n",
       "23                 society       0.223155       0.641169    0.418013\n",
       "259       sars-cov-2 virus       0.466042       0.879672    0.413631\n",
       "37               influence       0.268647       0.682028    0.413380\n",
       "352            fundamental       0.490767       0.903265    0.412498\n",
       "42                    fear       0.293552       0.705589    0.412037\n",
       "86              politician       0.373746       0.783666    0.409920\n",
       "2346                  test       0.691577       0.289005    0.402572\n",
       "93                     flu       0.387179       0.781119    0.393941\n",
       "34                  invest       0.259269       0.650016    0.390747\n",
       "2593                 fever       0.832818       0.442447    0.390371\n",
       "2232              genetics       0.668181       0.281013    0.387168\n",
       "28                 economy       0.243591       0.629304    0.385713\n",
       "647                 unfold       0.527646       0.910649    0.383002\n",
       "40          global vaccine       0.289448       0.672388    0.382940\n",
       "44                     way       0.296122       0.678856    0.382735\n",
       "57         health security       0.321791       0.703496    0.381705\n",
       "50                   force       0.312763       0.693235    0.380472\n",
       "795       disease outbreak       0.540486       0.920340    0.379854\n",
       "159                  logic       0.433504       0.810863    0.377358\n",
       "26                 country       0.242355       0.619393    0.377038\n",
       "63             good health       0.338167       0.714336    0.376169\n",
       "43               emergency       0.294177       0.666495    0.372318\n",
       "41        gates foundation       0.292785       0.663574    0.370789"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6=df4.merge(df5,on='Term')\n",
    "df6['Difference']=abs(df6['Distance_true']-df6['Distance_fake'])\n",
    "df6.sort_values(by='Difference', ascending=False, inplace=True)\n",
    "df6.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check if the fake news contains terms that are semantically distant from the meaning of the term 'disease' in Wordnet, by calculating the Wu and Palmer similarity between the wordnet sense 'disease.n.01' and the terms related to 'coronavirus' in the word2vec model of fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We calculate the Wu and Palmer distance between the sense 'disease.n.01' and the first sense of the nouns most closely related to 'coronavirus' in the word2vec model of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu and Palmer distance between 'gate' and 'disease(n.01)':  0.1\n",
      "The term 'roseland_community' is not in WordNet \n",
      "Wu and Palmer distance between 'government' and 'disease(n.01)':  0.23529411764705882\n",
      "The term 'investigational_drugs' is not in WordNet \n",
      "Wu and Palmer distance between 'take' and 'disease(n.01)':  0.2\n",
      "Wu and Palmer distance between 'system' and 'disease(n.01)':  0.11764705882352941\n",
      "Wu and Palmer distance between 'u' and 'disease(n.01)':  0.18181818181818182\n",
      "The term 'bad_news' is not in WordNet \n",
      "Wu and Palmer distance between 'control' and 'disease(n.01)':  0.375\n",
      "The term 'docuseries' is not in WordNet \n",
      "The term 'non-profit' is not in WordNet \n",
      "Wu and Palmer distance between 'foundation' and 'disease(n.01)':  0.2857142857142857\n",
      "Wu and Palmer distance between 'vaccine' and 'disease(n.01)':  0.11764705882352941\n",
      "Wu and Palmer distance between 'people' and 'disease(n.01)':  0.2857142857142857\n",
      "Wu and Palmer distance between 'salvation' and 'disease(n.01)':  0.2222222222222222\n",
      "Wu and Palmer distance between 'power' and 'disease(n.01)':  0.4\n",
      "Wu and Palmer distance between 'missile' and 'disease(n.01)':  0.1\n",
      "The term 'melinda_gates' is not in WordNet \n",
      "Wu and Palmer distance between 'state' and 'disease(n.01)':  0.1111111111111111\n",
      "The term 'immunity_passport' is not in WordNet \n",
      "The term 'outrageous' is not in WordNet \n",
      "The term 'chose' is not in WordNet \n",
      "The term 'economic_collapse' is not in WordNet \n",
      "The term 'much_lower' is not in WordNet \n",
      "Wu and Palmer distance between 'vaccination' and 'disease(n.01)':  0.21052631578947367\n",
      "The term 'government_agencies' is not in WordNet \n",
      "The term 'disciplinary_methods' is not in WordNet \n",
      "The term 'public_immunization' is not in WordNet \n",
      "The term 'profusa' is not in WordNet \n",
      "Wu and Palmer distance between 'closing' and 'disease(n.01)':  0.21052631578947367\n",
      "The term 'vaccination_id' is not in WordNet \n",
      "The term 'global_alliance' is not in WordNet \n",
      "The term 'atrocious' is not in WordNet \n",
      "Wu and Palmer distance between 'jab' and 'disease(n.01)':  0.2\n",
      "The term 'includes_tagging' is not in WordNet \n",
      "The term 'global_capitalism' is not in WordNet \n",
      "Wu and Palmer distance between 'life' and 'disease(n.01)':  0.5\n",
      "The term 'digital' is not in WordNet \n",
      "The term 'powerful_man' is not in WordNet \n",
      "Wu and Palmer distance between 'public' and 'disease(n.01)':  0.26666666666666666\n",
      "The term 'greater_control' is not in WordNet \n",
      "Wu and Palmer distance between 'world' and 'disease(n.01)':  0.125\n",
      "The term 'microsoft_co-founder' is not in WordNet \n",
      "The term 'mass_gatherings' is not in WordNet \n",
      "The term 'up-voted_response' is not in WordNet \n",
      "Wu and Palmer distance between 'society' and 'disease(n.01)':  0.26666666666666666\n",
      "The term 'unitaid' is not in WordNet \n",
      "Wu and Palmer distance between 'third_world' and 'disease(n.01)':  0.26666666666666666\n",
      "Wu and Palmer distance between 'puppet' and 'disease(n.01)':  0.1\n",
      "Wu and Palmer distance between 'toll' and 'disease(n.01)':  0.19047619047619047\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    term=df4.Term.iloc[i]\n",
    "    try:\n",
    "        term=term.strip().replace(' ', '_')\n",
    "        sense_term=wn.synset(term+'.n.01')\n",
    "        print(\"Wu and Palmer distance between '{}' and 'disease(n.01)': \".format(term),\n",
    "            sense_term.wup_similarity(wn.synset('disease.n.01')))\n",
    "    except: \n",
    "        print(\"The term '{}' is not in WordNet \".format(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wu and Palmer similarity is a score from from 0 (more similar) to 1 (less similar).\n",
    "We can see that a lot of the terms that are semantically close to 'coronavirus' in the fake news set have a score really low when measuring the similarity with the synset \"disease.n.01\", that should be higher considering that 'coronavirus' is a disease. A lot of this terms are also missing from Wordnet.\n",
    "\n",
    "The reason for this may be that Wordnet needs constatly to be updated with new terms, with all the new semantic relations. For this reason I don't think that it is a good resource to analyze news about covid-19, that is a quite recent topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We build an automatic binary classifier between true and fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoTitle</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NoTitle</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NoTitle</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoTitle</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                            NoTitle   \n",
       "2                                            NoTitle   \n",
       "3                                            NoTitle   \n",
       "4                                            NoTitle   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"corona_fake.csv\")\n",
    "df.label = df.label.replace(['fake'],'Fake')\n",
    "#we fill the missing values (insted of dropping) because the info of \"there is no title or text\"\n",
    "#could be useful to detect fake news\n",
    "df['title']=df.title.fillna('NoTitle')\n",
    "df['text']=df.text.fillna('NoText')\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.title.values #we use the title as the feature\n",
    "y = df.label.values #we use the label as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the titles suing TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#we set lowercase=False because capital letters may be a distinctive element of the classes\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=False)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf.A, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score on the test set is: 0.8407310704960835\n"
     ]
    }
   ],
   "source": [
    "#predictions and evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "# Let's see how the model performs on the test set\n",
    "score = accuracy_score(y_test, pred)\n",
    "print('The accuracy score on the test set is: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake\n",
      "(-0.8280698971999038, 'NoTitle')\n",
      "(-0.5341399179348005, 'Vitamin')\n",
      "(-0.4128925346426516, 'Update')\n",
      "(-0.3288142420480259, '5G')\n",
      "(-0.30132923982040827, 'bioweapon')\n",
      "(-0.299866574540067, 'Word')\n",
      "(-0.23802049868823077, 'Behind')\n",
      "(-0.21299674254185005, 'Wuhan')\n",
      "(-0.20278850293745967, 'caused')\n",
      "(-0.18952930248806424, 'Treatment')\n",
      "(-0.1877335854993494, 'CORONAVIRUS')\n",
      "(-0.18623578147314312, 'The')\n",
      "(-0.18221589243817743, 'Cheap')\n",
      "(-0.18219215200947916, 'Plandemic')\n",
      "(-0.1814283832637579, 'Nutritional')\n",
      "(-0.18056431366271247, 'Piracy')\n",
      "(-0.17984690657501196, 'United')\n",
      "(-0.17984690657501035, 'States')\n",
      "(-0.16794374856849092, 'Made')\n",
      "(-0.16584020929532958, 'consuming')\n",
      "\n",
      "TRUE\n",
      "(0.1696355508460357, 'originated')\n",
      "(0.17674354625864708, 'Should')\n",
      "(0.1779352544051644, 'Pandemic')\n",
      "(0.17916062741181765, 'consumption')\n",
      "(0.17940054686631635, 'Symptoms')\n",
      "(0.1819818601051003, 'know')\n",
      "(0.18218565343116452, 'When')\n",
      "(0.1828711453304146, 'virus')\n",
      "(0.1995257121715009, 'disease')\n",
      "(0.2203316996138784, 'Safe')\n",
      "(0.22398230291026255, 'outbreak')\n",
      "(0.22921486045270262, 'does')\n",
      "(0.23779285629052366, 'coronavirus')\n",
      "(0.2481843019730501, 'Coronavirus')\n",
      "(0.2537118574329505, 'symptoms')\n",
      "(0.26431702867857354, 'new')\n",
      "(0.27078397599454973, 'COVID')\n",
      "(0.27121227032274703, '19')\n",
      "(0.3746968286584459, 'How')\n",
      "(0.5446589064593106, 'What')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#let's inspect the most informative words for the model\n",
    "\n",
    "class_labels = lr.classes_\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(lr.coef_[0], feature_names))\n",
    "\n",
    "print(class_labels[0]) \n",
    "for f in feat_with_weights[:20]:\n",
    "    print(f)\n",
    "    \n",
    "print('')\n",
    "\n",
    "print(class_labels[1]) \n",
    "for f in feat_with_weights[-20:]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most informative feature of the titles of fake news is its absence ('NoTitle' has weight -0.8178656394286765). \n",
    "\n",
    "Also the word \"CORONAVIRUS\" when written all in uppercase is distinctive of the fake news, while in the the true news it is written as 'Coronavirus' or 'coronavirus'.\n",
    "\n",
    "There also can be found other terms that are related to conspiracy theories or pseudoscience in the fake news most informative words as '5G', 'Vitamin' or 'bioweapon'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classifier\n",
    "\n",
    "We will build a more complex binary classifier, compsed by an embedding layer (weights taken from Glove), a LSTM layer with 300 units and at the end a fully connected layer, with output dimension = 2 (True/Fake).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Piracy everywhere and pandemic for all</td>\n",
       "      <td>The crisis originated less in a miserable anim...</td>\n",
       "      <td>http://www.egaliteetreconciliation.fr</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Scams related to COVID-19</td>\n",
       "      <td>COVID-19: Commission and national consumer aut...</td>\n",
       "      <td>https://ec.europa.eu/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>I do not believe this is accidental\": expert o...</td>\n",
       "      <td>Topical comments on the state of the Chinese e...</td>\n",
       "      <td>https://sputnik.by/</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>That ‘Miracle Cure’ You Saw on Facebook? It Wo...</td>\n",
       "      <td>Gargling warm salty water, taking vitamins or ...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>The COVID-19 pandemic could last for 2 years, ...</td>\n",
       "      <td>A new report from researchers at the Center fo...</td>\n",
       "      <td>https://www.weforum.org/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>How does soap kill coronavirus? If I don’t hav...</td>\n",
       "      <td>Yes, you can use soap and water on surfaces ju...</td>\n",
       "      <td>https://www.cnn.com/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>empty</td>\n",
       "      <td>The patented nanosilver we have, the Pentagon ...</td>\n",
       "      <td>infowars.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Viral video mixes truth about COVID-19 with a ...</td>\n",
       "      <td>Runny nose and sputum are indeed symptoms of t...</td>\n",
       "      <td>https://healthfeedback.org/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>What are super spreaders and how can they affe...</td>\n",
       "      <td>Rather than using the term super spreaders (a ...</td>\n",
       "      <td>https://www.globalhealthnow.org/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>How long does coronavirus live on surfaces?</td>\n",
       "      <td>Which surfaces are the most infectious and how...</td>\n",
       "      <td>https://www.nbcnews.com/</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "212              Piracy everywhere and pandemic for all   \n",
       "597                           Scams related to COVID-19   \n",
       "255   I do not believe this is accidental\": expert o...   \n",
       "776   That ‘Miracle Cure’ You Saw on Facebook? It Wo...   \n",
       "694   The COVID-19 pandemic could last for 2 years, ...   \n",
       "1106  How does soap kill coronavirus? If I don’t hav...   \n",
       "67                                                empty   \n",
       "774   Viral video mixes truth about COVID-19 with a ...   \n",
       "840   What are super spreaders and how can they affe...   \n",
       "1027        How long does coronavirus live on surfaces?   \n",
       "\n",
       "                                                   text  \\\n",
       "212   The crisis originated less in a miserable anim...   \n",
       "597   COVID-19: Commission and national consumer aut...   \n",
       "255   Topical comments on the state of the Chinese e...   \n",
       "776   Gargling warm salty water, taking vitamins or ...   \n",
       "694   A new report from researchers at the Center fo...   \n",
       "1106  Yes, you can use soap and water on surfaces ju...   \n",
       "67    The patented nanosilver we have, the Pentagon ...   \n",
       "774   Runny nose and sputum are indeed symptoms of t...   \n",
       "840   Rather than using the term super spreaders (a ...   \n",
       "1027  Which surfaces are the most infectious and how...   \n",
       "\n",
       "                                     source label  \n",
       "212   http://www.egaliteetreconciliation.fr  Fake  \n",
       "597                   https://ec.europa.eu/  TRUE  \n",
       "255                     https://sputnik.by/  Fake  \n",
       "776                https://www.nytimes.com/  TRUE  \n",
       "694                https://www.weforum.org/  TRUE  \n",
       "1106                   https://www.cnn.com/  TRUE  \n",
       "67                             infowars.com  Fake  \n",
       "774             https://healthfeedback.org/  TRUE  \n",
       "840        https://www.globalhealthnow.org/  TRUE  \n",
       "1027               https://www.nbcnews.com/  TRUE  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_na.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Due to the recent outbreak for the Coronavirus (COVID-19) the World Health Organization is giving away vaccine kits. Just pay $4.95 for shipping'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_na.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You just need to add water, and the drugs and vaccines are ready to be administered. There are two parts to the kit: one holds pellets containing the chemical machinery that synthesises the end product, and the other holds pellets containing instructions that telll the drug which compound to create. Mix two parts together in a chosen combination, add water, and the treatment is ready.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_na.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_no_na.title.tolist()\n",
    "texts = df_no_na.text.tolist()\n",
    "labels = df_no_na.label.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "print(integer_encoded)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you just need to add water and the drugs and vaccines are ready to be administered there are two parts to the kit one holds pellets containing the chemical machinery that synthesises the end product and the other holds pellets containing instructions that telll the drug which compound to create mix two parts together in a chosen combination add water and the treatment is ready', 'hydroxychloroquine has been shown to have a 100 effective rate treating covid19 yet democrat gretchen whitmer is threatening doctors who prescribe it if trump is for something democrats are against it they are okay with people dying if it means opposing trump', 'fact hydroxychloroquine has been shown to have a 100 effective rate treating covid19 yet democrat gretchen whitmer is threatening doctors who prescribe it if trump is for something democrats are against it they are okay with people dying if it means opposing trump sick', 'the corona virus is a man made virus created in a wuhan laboratory ask billgates who financed it', 'doesn’t billgates finance research at the wuhan lab where the corona virus was being created isn’t georgesoros a good friend of gates', 'urgent health bulletin to the public ministry of health’s emergency notification to the public that the coronavirus outbreak this time is very very serious  fatal there’s no cure once you are infected its spreading from china to various countries prevention method is to keep your throat moist do not let your throat dry up thus do not hold your thirst because once your membrane in your throat is dried the virus will invade into your body within 10 mins drink 5080cc warm water 3050cc for kids according to age everytime u feel your throat is dry do not wait keep water in hand do not drink plenty at one time as it doesn’t help instead continue to keep throat moist till end of march 2020 do not go to crowded places wear mask as needed especially in train or public transportation avoid fried or spicy food and load up vitamin c the symptoms description are 1repeated high fever 2prolonged coughing after fever\\n3children are prone 4adults usually feel uneasy headache and mainly respiratory related 5 highly contagious please forward to help others', 'pls tell ur families relatives and friendsmoh health bulletin to the public the upper respiratory infection affecting china at present is quite serious the virus causing it is very potent and is resistant to existing antibiotics virus is not bacterial infection hence cannot be treated by antibiotics the prevention method now is to keep your throat moist do not let your throat dry up thus do not hold your thirst because once your membrane in your throat is dried the virus will invade into your body within 10 mins drink 5080cc warm water 3050cc for kids according to age everytime sic you feel your throat is dry do not wait keep water in hand do not drink plenty at one time as it does not help instead continue to keep throat moist till end of march do not go to crowded places wear mask as needed especially in train or public transportation avoid fried or spicy food and load up vitamin c the symptomsdescription are repeated high fever 2 prolonged coughing after fever 3 children are more prone 4 adults usually feel uneasy headache and mainly respiratory related illness this illness is highly contagious let’s continue to pray and wait for further notice about the infection please share ', 'serious excellent advice by japanese doctors treating covid19 cases\\xa0 everyone should ensure your mouth  throat is moist never dry take a few sips of water every 15 mins at least why\\xa0 even if the virus gets into your mouth … drinking water or other liquids will wash them down through your oesophagus and into the stomach once there in tummy … your stomach acid will kill all the virus if you don’t drink enough water more regularly … the virus can enter your windpipes and into the lungs that’s very dangerous', 'the new coronavirus may not show signs of infection for many days how can you know if you are infected by the time you have fever andor cough and go to the hospital the lung is usually 50 fibrosis taiwan experts provide a simple selfcheck that we can do every morning take a deep breath and hold it for more than 10 seconds if you do this successfully without coughing without discomfort stiffness or tightness there is no fibrosis in the lungs it basically indicates no infection in critical times please selfcheck every morning in an environment with clean air', 'a vaccine meant for cattle can be used to fight covid19']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Toss out punctuation\n",
    "data = [s.translate(str.maketrans('', '', string.punctuation)) for s in texts]\n",
    "\n",
    "# Convert to lower case\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].lower()\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 37299\n"
     ]
    }
   ],
   "source": [
    "def tokenization(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer\n",
    "\n",
    "data_tokenizer = tokenization(data)\n",
    "\n",
    "vocab_size = len(data_tokenizer.word_index) + 1\n",
    "\n",
    "print('Vocabulary Size: %d' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "glove_path = 'glove.42B.300d.txt'\n",
    "f = open(glove_path)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(data_tokenizer.word_index) + 1, 300))\n",
    "for word, i in data_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37299, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the embedding layer\n",
    "\n",
    "embedding_vec_length = 300\n",
    "max_len = max([len(x.split()) for x in data])\n",
    "\n",
    "embedding_layer = Embedding(len(data_tokenizer.word_index) + 1,\n",
    "                            embedding_vec_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5184"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use just the bodies as features\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, onehot_encoded, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # encode the sequences with word indexes\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # padding\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq\n",
    "\n",
    "X_train = encode_sequences(data_tokenizer, max_len, X_train)\n",
    "X_test = encode_sequences(data_tokenizer, max_len, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(927, 5184)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 300\n",
    "\n",
    "def build_model_classification(num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "cl_model = build_model_classification(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "24/24 [==============================] - 773s 32s/step - loss: 0.6415 - accuracy: 0.6329 - val_loss: 0.5655 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56552, saving model to cl_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "24/24 [==============================] - 715s 30s/step - loss: 0.4785 - accuracy: 0.7787 - val_loss: 0.4851 - val_accuracy: 0.7688\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56552 to 0.48507, saving model to cl_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199897080>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'cl_model' #file name to save the model\n",
    "\n",
    "#ModelCheckpoint saves the model with the best validation loss (best epoch)\n",
    "\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "cl_model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2, callbacks=[checkpoint], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "24/24 [==============================] - 737s 31s/step - loss: 0.3511 - accuracy: 0.8529 - val_loss: 0.5441 - val_accuracy: 0.7634\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.48507\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 710s 30s/step - loss: 0.3504 - accuracy: 0.8516 - val_loss: 0.5578 - val_accuracy: 0.7688\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.48507\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 754s 32s/step - loss: 0.2426 - accuracy: 0.8920 - val_loss: 0.4880 - val_accuracy: 0.8226\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48507\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 854s 36s/step - loss: 0.1841 - accuracy: 0.9258 - val_loss: 0.4819 - val_accuracy: 0.8280\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48507 to 0.48186, saving model to cl_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cl_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "24/24 [==============================] - 762s 32s/step - loss: 0.1438 - accuracy: 0.9541 - val_loss: 0.5147 - val_accuracy: 0.7957\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1789e5390>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, callbacks=[checkpoint], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cl_model = load_model('cl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78       117\n",
      "           1       0.74      0.96      0.83       115\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       232\n",
      "   macro avg       0.84      0.81      0.81       232\n",
      "weighted avg       0.84      0.81      0.81       232\n",
      " samples avg       0.81      0.81      0.81       232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = cl_model.predict(X_test)\n",
    "class_predicted = np.where(y_pred > 0.5, 1,0)\n",
    "print(classification_report(y_test, class_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 87s 11s/step - loss: 0.5422 - accuracy: 0.8103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.542235791683197, 0.8103448152542114]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x171939eb8>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "Ytest_int = np.argmax(y_test, axis=1)\n",
    "class_pred_int=np.argmax(class_predicted,axis=1)\n",
    "ConfusionMatrixDisplay.from_predictions(Ytest_int, class_pred_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
